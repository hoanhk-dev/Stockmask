{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b42721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage\n",
    ")\n",
    "import operator\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_URL = os.getenv(\"LLM_URL\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2679f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    url: str\n",
    "    title: Optional[str] = None\n",
    "    snippet: Optional[str] = None\n",
    "    engine: Optional[str] = None\n",
    "    score: Optional[float] = None\n",
    "    extracted_at: str\n",
    "class SearchError(BaseModel):\n",
    "    query: str\n",
    "    error: str\n",
    "    extracted_at: str\n",
    "    success: bool = False\n",
    "\n",
    "class SearxNGSearch:\n",
    "    def __init__(self, base_url: Optional[str] = None):\n",
    "        self.base_url = base_url or os.getenv(\"SEARXNG_BASE_URL\", \"http://localhost:8888/search\")\n",
    "        if not self.base_url:\n",
    "            raise ValueError(\"SEARXNG_BASE_URL is not set\")\n",
    "\n",
    "    async def _fetch_page(\n",
    "        self,\n",
    "        client: httpx.AsyncClient,\n",
    "        query: str,\n",
    "        page: int,\n",
    "        timeout: int,\n",
    "    ) -> Union[List[dict], str]:\n",
    "        try:\n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"format\": \"json\",\n",
    "                \"pageno\": page,\n",
    "            }\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "                \"Accept\": \"application/json\",\n",
    "            }\n",
    "            r = await client.get(self.base_url, params=params, timeout=timeout, headers=headers)\n",
    "            r.raise_for_status()\n",
    "            return r.json().get(\"results\", [])\n",
    "        except Exception as e:\n",
    "            return f\"Page {page} failed: {e}\"\n",
    "\n",
    "    async def _search_async(\n",
    "        self,\n",
    "        query: str,\n",
    "        max_pages: int,\n",
    "        timeout: int,\n",
    "        max_results: Optional[int],\n",
    "        output_for_llm: bool,\n",
    "    ) -> Union[List[SearchResult], str, SearchError]:\n",
    "\n",
    "        collected: List[SearchResult] = []\n",
    "\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            tasks = [\n",
    "                self._fetch_page(client, query, page, timeout)\n",
    "                for page in range(1, max_pages + 1)\n",
    "            ]\n",
    "            pages = await asyncio.gather(*tasks)\n",
    "\n",
    "        for page in pages:\n",
    "            if isinstance(page, str):\n",
    "                return SearchError(\n",
    "                    query=query,\n",
    "                    error=page,\n",
    "                    extracted_at=datetime.now().isoformat(),\n",
    "                )\n",
    "\n",
    "            for item in page:\n",
    "                collected.append(\n",
    "                    SearchResult(\n",
    "                        url=item.get(\"url\"),\n",
    "                        title=item.get(\"title\"),\n",
    "                        snippet=item.get(\"content\"),  # SearxNG snippet\n",
    "                        engine=item.get(\"engine\"),\n",
    "                        score=item.get(\"score\"),\n",
    "                        extracted_at=datetime.now().isoformat(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if max_results:\n",
    "            collected.sort(\n",
    "                key=lambda x: x.score if x.score is not None else 0,\n",
    "                reverse=True,\n",
    "            )\n",
    "            collected = collected[:max_results]\n",
    "\n",
    "        if output_for_llm:\n",
    "            return \"\\n\".join(r.model_dump_json() for r in collected)\n",
    "\n",
    "        return collected\n",
    "\n",
    "    async def asearch(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        max_pages: int = 3,\n",
    "        timeout: int = 10,\n",
    "        max_results: Optional[int] = None,\n",
    "        output_for_llm: bool = False,\n",
    "    ):\n",
    "        return await self._search_async(\n",
    "            query=query,\n",
    "            max_pages=max_pages,\n",
    "            timeout=timeout,\n",
    "            max_results=max_results,\n",
    "            output_for_llm=output_for_llm,\n",
    "        )\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        max_pages: int = 3,\n",
    "        timeout: int = 10,\n",
    "        max_results: Optional[int] = None,\n",
    "        output_for_llm: bool = False,\n",
    "    ) -> Union[List[SearchResult], str, SearchError]:\n",
    "\n",
    "        return asyncio.run(\n",
    "            self._search_async(\n",
    "                query=query,\n",
    "                max_pages=max_pages,\n",
    "                timeout=timeout,\n",
    "                max_results=max_results,\n",
    "                output_for_llm=output_for_llm,\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f069d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    company: str\n",
    "    indicator: str\n",
    "\n",
    "class Controller:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            base_url=LLM_URL,      \n",
    "            api_key=OPENAI_API_KEY,  \n",
    "            model=LLM_MODEL,\n",
    "            temperature=0.2\n",
    "        )\n",
    "    \n",
    "        self.app = self.build_graph()\n",
    "        self.searcher = SearxNGSearch()\n",
    "\n",
    "    async def llm_node(self, state: AgentState):\n",
    "        company = state[\"company\"]\n",
    "\n",
    "        system_prompt = SystemMessage(\n",
    "            content=(\n",
    "                \"You are a web relevance evaluation expert.\\n\"\n",
    "                \"Your task is to evaluate whether each provided URL is directly related \"\n",
    "                \"to the given Japanese company.\\n\\n\"\n",
    "                \"Evaluation rules:\\n\"\n",
    "                \"- Consider official websites, subsidiaries, IR pages, press releases, \"\n",
    "                \"and credible news sources as relevant.\\n\"\n",
    "                \"- Mark unrelated companies, generic blogs, forums, or ads as not relevant.\\n\"\n",
    "                \"- Be concise and objective.\\n\\n\"\n",
    "                \"Output format:\\n\"\n",
    "                \"- URL\\n\"\n",
    "                \"- Relevance: High / Medium / Low / None\\n\"\n",
    "                \"- Short justification (1 sentence)\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        urls_text = \"\\n\".join(\n",
    "            msg.content for msg in state[\"messages\"] if isinstance(msg, HumanMessage)\n",
    "        )\n",
    "        \n",
    "        print(urls_text)\n",
    "\n",
    "        human_prompt = HumanMessage(\n",
    "            content=(\n",
    "                f\"Company: {company}\\n\\n\"\n",
    "                \"URLs to evaluate:\\n\"\n",
    "                f\"{urls_text}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        messages = [system_prompt, human_prompt]\n",
    "\n",
    "        response = await self.llm.ainvoke(messages)\n",
    "\n",
    "        return {\n",
    "            \"messages\": [response.content],\n",
    "        }\n",
    "\n",
    "    \n",
    "    async def search_node(\n",
    "        self,\n",
    "        state: AgentState,\n",
    "        max_pages: int = 3,\n",
    "        max_results: int = 5,\n",
    "    ):\n",
    "        company = state[\"company\"]\n",
    "        indicator = state[\"indicator\"]\n",
    "\n",
    "        query = f\"{company} {indicator}\"\n",
    "        print(f\"[SEARCH] Query: {query}\")\n",
    "\n",
    "        results = await self.searcher.asearch(\n",
    "            query=query,\n",
    "            max_pages=max_pages,\n",
    "            max_results=max_results,\n",
    "            output_for_llm=False,\n",
    "        )\n",
    "        print(\"results\",results)\n",
    "\n",
    "        urls = []\n",
    "        for r in results:\n",
    "            urls.append(r.url)\n",
    "\n",
    "        urls_text = \"\\n\".join(urls)\n",
    "        print(\"urls_text\",urls_text)\n",
    "\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=(\n",
    "                        \"The following URLs were retrieved from web search:\\n\"\n",
    "                        f\"{urls_text}\"\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            \"search_results\": results,\n",
    "        }\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        graph = StateGraph(AgentState)\n",
    "\n",
    "        graph.add_node(\"searcher\", self.search_node)\n",
    "        graph.add_node(\"llm_node\", self.llm_node)\n",
    "\n",
    "        graph.set_entry_point(\"searcher\")\n",
    "        graph.add_edge(\"searcher\", \"llm_node\")\n",
    "        graph.add_edge(\"llm_node\", END)\n",
    "\n",
    "        app = graph.compile()\n",
    "\n",
    "        return app\n",
    "    \n",
    "    async def run(self, user_input: str, company: str, indicator: str):\n",
    "        initial_state: AgentState = {\n",
    "            \"messages\": [HumanMessage(content=user_input)] if user_input else [],\n",
    "            \"company\": company,\n",
    "            \"indicator\": indicator,\n",
    "        }\n",
    "\n",
    "        result = await self.app.ainvoke(initial_state)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30102d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEARCH] Query: 鹿島建設 株主優待の有無・内容\n",
      "results []\n",
      "urls_text \n",
      "The following URLs were retrieved from web search:\n",
      "\n",
      "I notice that you've mentioned \"The following URLs were retrieved from web search:\" but haven't provided any actual URLs for evaluation. \n",
      "\n",
      "To properly assess the relevance of URLs to 鹿島建設 (Kajima Corporation), I need the specific URLs to analyze. Please provide the list of URLs you'd like me to evaluate for their relevance to this Japanese construction company.\n",
      "\n",
      "Once you share the URLs, I'll evaluate each one based on whether it's directly related to 鹿島建設, considering factors like official websites, subsidiaries, IR pages, press releases, and credible news sources.\n"
     ]
    }
   ],
   "source": [
    "controller = Controller()\n",
    "\n",
    "user_input = \"\"\n",
    "company = \"鹿島建設\"\n",
    "indicator = \"株主優待の有無・内容\"\n",
    "\n",
    "result = await controller.run(\n",
    "    user_input = user_input,\n",
    "    company=company,\n",
    "    indicator=indicator\n",
    ")\n",
    "last_message = result[\"messages\"][-1]\n",
    "print(last_message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
